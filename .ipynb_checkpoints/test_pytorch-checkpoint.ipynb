{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for testing pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new neural network\n",
    "class BinaryClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a neural network that performs binary classification.\n",
    "    The network should accept your number of features as input, and produce \n",
    "    a single sigmoid value, that can be rounded to a label: 0 or 1, as output.\n",
    "    \n",
    "    Notes on training:\n",
    "    To train a binary classifier in PyTorch, use BCELoss.\n",
    "    BCELoss is binary cross entropy loss, documentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO: Define the init function, the input params are required (for loading code in train.py to work)\n",
    "    def __init__(self, input_features, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up linear layers.\n",
    "        Use the input parameters to help define the layers of your model.\n",
    "        :param input_features: the number of input features in your training/test data\n",
    "        :param hidden_dim: helps define the number of nodes in the hidden layer(s)\n",
    "        :param output_dim: the number of outputs you want to produce\n",
    "        \"\"\"\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        # define any initial layers, here\n",
    "        self.fc1 = nn.Linear(input_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    \n",
    "    ## TODO: Define the feedforward behavior of the network\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on input features, x.\n",
    "        :param x: A batch of input features of size (batch_size, input_features)\n",
    "        :return: A single, sigmoid-activated value as output\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the feedforward behavior\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.drop(out)\n",
    "        out = F.sigmoid(self.fc2(out))\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided training function\n",
    "def train(model, train_loader, epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the PyTorch training script. The parameters\n",
    "    passed are as follows:\n",
    "    model        - The PyTorch model that we wish to train.\n",
    "    train_loader - The PyTorch DataLoader that should be used during training.\n",
    "    epochs       - The total number of epochs to train for.\n",
    "    criterion    - The loss function used for training. \n",
    "    optimizer    - The optimizer to use during training.\n",
    "    device       - Where the model and data should be loaded (gpu or cpu).\n",
    "    \"\"\"\n",
    "    \n",
    "    # training loop is provided\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Make sure that the model is in training mode.\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # get data\n",
    "            batch_x, batch_y = batch\n",
    "\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get predictions from model\n",
    "            y_pred = model(batch_x)\n",
    "            \n",
    "            # perform backprop\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        \n",
    "    print(\"Final-Loss = {};\".format(total_loss / len(train_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets training data in batches from the train.csv file\n",
    "def _get_train_data_loader(batch_size, training_dir):\n",
    "    print(\"Get train data loader.\")\n",
    "\n",
    "    train_data = pd.read_csv(os.path.join(training_dir, \"train.csv\"), header=None, names=None)\n",
    "\n",
    "    train_y = torch.from_numpy(train_data[[0]].values).float().squeeze()\n",
    "    train_x = torch.from_numpy(train_data.drop([0], axis=1).values).float()\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "\n",
    "    return torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up arguments for training\n",
    "\n",
    "seed = 1\n",
    "batch_size = 10\n",
    "data_dir = 'plagiarism_data'\n",
    "\n",
    "input_features = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "epochs = 300\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n",
      "Get train data loader.\n",
      "Epoch: 1, Loss: 0.7455229503767831\n",
      "Epoch: 2, Loss: 0.7501341274806431\n",
      "Epoch: 3, Loss: 0.741145099912371\n",
      "Epoch: 4, Loss: 0.7178898027965\n",
      "Epoch: 5, Loss: 0.7174884762082782\n",
      "Epoch: 6, Loss: 0.716934791633061\n",
      "Epoch: 7, Loss: 0.7060960275786263\n",
      "Epoch: 8, Loss: 0.7024428248405457\n",
      "Epoch: 9, Loss: 0.6792037827627999\n",
      "Epoch: 10, Loss: 0.677945077419281\n",
      "Epoch: 11, Loss: 0.6828325475965228\n",
      "Epoch: 12, Loss: 0.6737110614776611\n",
      "Epoch: 13, Loss: 0.6453953215054103\n",
      "Epoch: 14, Loss: 0.6510436023984637\n",
      "Epoch: 15, Loss: 0.659871016229902\n",
      "Epoch: 16, Loss: 0.6501941340310233\n",
      "Epoch: 17, Loss: 0.6299493483134678\n",
      "Epoch: 18, Loss: 0.6361554350171771\n",
      "Epoch: 19, Loss: 0.6222763231822422\n",
      "Epoch: 20, Loss: 0.6212299977030072\n",
      "Epoch: 21, Loss: 0.6112787042345319\n",
      "Epoch: 22, Loss: 0.5991738949503217\n",
      "Epoch: 23, Loss: 0.6030174153191703\n",
      "Epoch: 24, Loss: 0.6068858163697379\n",
      "Epoch: 25, Loss: 0.5958078673907689\n",
      "Epoch: 26, Loss: 0.5781789336885724\n",
      "Epoch: 27, Loss: 0.5834511688777378\n",
      "Epoch: 28, Loss: 0.5729913881846836\n",
      "Epoch: 29, Loss: 0.5976804196834564\n",
      "Epoch: 30, Loss: 0.5746380516460964\n",
      "Epoch: 31, Loss: 0.5654888153076172\n",
      "Epoch: 32, Loss: 0.5740670136043003\n",
      "Epoch: 33, Loss: 0.5798241964408329\n",
      "Epoch: 34, Loss: 0.5563842526503971\n",
      "Epoch: 35, Loss: 0.5543764148439679\n",
      "Epoch: 36, Loss: 0.5209640477384839\n",
      "Epoch: 37, Loss: 0.5486314424446651\n",
      "Epoch: 38, Loss: 0.545461084161486\n",
      "Epoch: 39, Loss: 0.5423568657466343\n",
      "Epoch: 40, Loss: 0.5316349565982819\n",
      "Epoch: 41, Loss: 0.556970374924796\n",
      "Epoch: 42, Loss: 0.5238190804209027\n",
      "Epoch: 43, Loss: 0.530416761125837\n",
      "Epoch: 44, Loss: 0.5060197242668697\n",
      "Epoch: 45, Loss: 0.5529120351587024\n",
      "Epoch: 46, Loss: 0.5263986885547638\n",
      "Epoch: 47, Loss: 0.5373461203915733\n",
      "Epoch: 48, Loss: 0.5094396982874189\n",
      "Epoch: 49, Loss: 0.526943462235587\n",
      "Epoch: 50, Loss: 0.5266426290784564\n",
      "Epoch: 51, Loss: 0.5233739955084664\n",
      "Epoch: 52, Loss: 0.5132033016000476\n",
      "Epoch: 53, Loss: 0.5286216650690351\n",
      "Epoch: 54, Loss: 0.49755964960370747\n",
      "Epoch: 55, Loss: 0.4938932401793344\n",
      "Epoch: 56, Loss: 0.5058324422155108\n",
      "Epoch: 57, Loss: 0.5057859122753143\n",
      "Epoch: 58, Loss: 0.4940574892929622\n",
      "Epoch: 59, Loss: 0.484537524836404\n",
      "Epoch: 60, Loss: 0.5259289400918143\n",
      "Epoch: 61, Loss: 0.5007688743727547\n",
      "Epoch: 62, Loss: 0.480281834091459\n",
      "Epoch: 63, Loss: 0.48037784014429363\n",
      "Epoch: 64, Loss: 0.5036480597087315\n",
      "Epoch: 65, Loss: 0.47582736185618807\n",
      "Epoch: 66, Loss: 0.4942788226263864\n",
      "Epoch: 67, Loss: 0.49055961200169157\n",
      "Epoch: 68, Loss: 0.4944585050855364\n",
      "Epoch: 69, Loss: 0.46326007161821636\n",
      "Epoch: 70, Loss: 0.4496719709464482\n",
      "Epoch: 71, Loss: 0.48418095282145907\n",
      "Epoch: 72, Loss: 0.4552432341235025\n",
      "Epoch: 73, Loss: 0.4414278141089848\n",
      "Epoch: 74, Loss: 0.46257037775857107\n",
      "Epoch: 75, Loss: 0.44616732001304626\n",
      "Epoch: 76, Loss: 0.4473235820020948\n",
      "Epoch: 77, Loss: 0.443616475377764\n",
      "Epoch: 78, Loss: 0.4393822763647352\n",
      "Epoch: 79, Loss: 0.4676171158041273\n",
      "Epoch: 80, Loss: 0.446127712726593\n",
      "Epoch: 81, Loss: 0.458326518535614\n",
      "Epoch: 82, Loss: 0.44152447155543734\n",
      "Epoch: 83, Loss: 0.4285370537212917\n",
      "Epoch: 84, Loss: 0.4671380179268973\n",
      "Epoch: 85, Loss: 0.44004808153424946\n",
      "Epoch: 86, Loss: 0.4270292946270534\n",
      "Epoch: 87, Loss: 0.41680443712643217\n",
      "Epoch: 88, Loss: 0.426595824105399\n",
      "Epoch: 89, Loss: 0.42133969920022146\n",
      "Epoch: 90, Loss: 0.44464916416576933\n",
      "Epoch: 91, Loss: 0.4419403203896114\n",
      "Epoch: 92, Loss: 0.43681222200393677\n",
      "Epoch: 93, Loss: 0.40779357297079905\n",
      "Epoch: 94, Loss: 0.4071757708277021\n",
      "Epoch: 95, Loss: 0.4212992361613682\n",
      "Epoch: 96, Loss: 0.40513344747679575\n",
      "Epoch: 97, Loss: 0.3983559012413025\n",
      "Epoch: 98, Loss: 0.41791631494249615\n",
      "Epoch: 99, Loss: 0.42047051446778433\n",
      "Epoch: 100, Loss: 0.4079462332384927\n",
      "Epoch: 101, Loss: 0.408025609595435\n",
      "Epoch: 102, Loss: 0.36884986077036175\n",
      "Epoch: 103, Loss: 0.43490150570869446\n",
      "Epoch: 104, Loss: 0.37555359516824993\n",
      "Epoch: 105, Loss: 0.39067926151411875\n",
      "Epoch: 106, Loss: 0.4064825177192688\n",
      "Epoch: 107, Loss: 0.41955702219690594\n",
      "Epoch: 108, Loss: 0.38659379737717764\n",
      "Epoch: 109, Loss: 0.3982740491628647\n",
      "Epoch: 110, Loss: 0.4308414374079023\n",
      "Epoch: 111, Loss: 0.39832973905972074\n",
      "Epoch: 112, Loss: 0.39179601839610506\n",
      "Epoch: 113, Loss: 0.3914563911301749\n",
      "Epoch: 114, Loss: 0.37992738825934275\n",
      "Epoch: 115, Loss: 0.4191577136516571\n",
      "Epoch: 116, Loss: 0.39000455822263447\n",
      "Epoch: 117, Loss: 0.3973967007228306\n",
      "Epoch: 118, Loss: 0.3600633612700871\n",
      "Epoch: 119, Loss: 0.3883168271609715\n",
      "Epoch: 120, Loss: 0.37398560983794077\n",
      "Epoch: 121, Loss: 0.38517951539584566\n",
      "Epoch: 122, Loss: 0.3698105386325291\n",
      "Epoch: 123, Loss: 0.3841814824513027\n",
      "Epoch: 124, Loss: 0.38146305084228516\n",
      "Epoch: 125, Loss: 0.38629564004284994\n",
      "Epoch: 126, Loss: 0.3564616356577192\n",
      "Epoch: 127, Loss: 0.37133545109203886\n",
      "Epoch: 128, Loss: 0.3517379696880068\n",
      "Epoch: 129, Loss: 0.34565262496471405\n",
      "Epoch: 130, Loss: 0.3462163954973221\n",
      "Epoch: 131, Loss: 0.3668026455811092\n",
      "Epoch: 132, Loss: 0.36653508458818707\n",
      "Epoch: 133, Loss: 0.3627765348979405\n",
      "Epoch: 134, Loss: 0.36367801257542204\n",
      "Epoch: 135, Loss: 0.38692194649151396\n",
      "Epoch: 136, Loss: 0.3412849817957197\n",
      "Epoch: 137, Loss: 0.3665467287812914\n",
      "Epoch: 138, Loss: 0.35847045694078716\n",
      "Epoch: 139, Loss: 0.34595912269183565\n",
      "Epoch: 140, Loss: 0.3365536034107208\n",
      "Epoch: 141, Loss: 0.3484205113989966\n",
      "Epoch: 142, Loss: 0.3360970424754279\n",
      "Epoch: 143, Loss: 0.3748496430260794\n",
      "Epoch: 144, Loss: 0.35431734153202604\n",
      "Epoch: 145, Loss: 0.3503616430929729\n",
      "Epoch: 146, Loss: 0.3378394714423588\n",
      "Epoch: 147, Loss: 0.3732990707669939\n",
      "Epoch: 148, Loss: 0.3133146997009005\n",
      "Epoch: 149, Loss: 0.3672389132635934\n",
      "Epoch: 150, Loss: 0.31888043880462646\n",
      "Epoch: 151, Loss: 0.33384014453206745\n",
      "Epoch: 152, Loss: 0.32004937529563904\n",
      "Epoch: 153, Loss: 0.33946997778756277\n",
      "Epoch: 154, Loss: 0.33572190574237276\n",
      "Epoch: 155, Loss: 0.33549592324665617\n",
      "Epoch: 156, Loss: 0.34959463136536734\n",
      "Epoch: 157, Loss: 0.33415728168828146\n",
      "Epoch: 158, Loss: 0.37729845302445547\n",
      "Epoch: 159, Loss: 0.3402522270168577\n",
      "Epoch: 160, Loss: 0.359820157289505\n",
      "Epoch: 161, Loss: 0.33290304030690876\n",
      "Epoch: 162, Loss: 0.3571338270391737\n",
      "Epoch: 163, Loss: 0.3536227898938315\n",
      "Epoch: 164, Loss: 0.3217973985842296\n",
      "Epoch: 165, Loss: 0.37019258737564087\n",
      "Epoch: 166, Loss: 0.3606692041669573\n",
      "Epoch: 167, Loss: 0.320326492190361\n",
      "Epoch: 168, Loss: 0.3905791086809976\n",
      "Epoch: 169, Loss: 0.3529820420912334\n",
      "Epoch: 170, Loss: 0.3157263376883098\n",
      "Epoch: 171, Loss: 0.3395987004041672\n",
      "Epoch: 172, Loss: 0.3484589798109872\n",
      "Epoch: 173, Loss: 0.3048975957291467\n",
      "Epoch: 174, Loss: 0.31570541858673096\n",
      "Epoch: 175, Loss: 0.31087683354105267\n",
      "Epoch: 176, Loss: 0.35531205790383474\n",
      "Epoch: 177, Loss: 0.32028300208704813\n",
      "Epoch: 178, Loss: 0.281652141894613\n",
      "Epoch: 179, Loss: 0.31893797857420786\n",
      "Epoch: 180, Loss: 0.2989886573382786\n",
      "Epoch: 181, Loss: 0.30783361409391674\n",
      "Epoch: 182, Loss: 0.30718848747866495\n",
      "Epoch: 183, Loss: 0.3115176090172359\n",
      "Epoch: 184, Loss: 0.3284500539302826\n",
      "Epoch: 185, Loss: 0.31941404725824085\n",
      "Epoch: 186, Loss: 0.33989943138190676\n",
      "Epoch: 187, Loss: 0.2912647937025343\n",
      "Epoch: 188, Loss: 0.3138932479279382\n",
      "Epoch: 189, Loss: 0.30765714177063536\n",
      "Epoch: 190, Loss: 0.3654740239892687\n",
      "Epoch: 191, Loss: 0.32827422448566984\n",
      "Epoch: 192, Loss: 0.2971545819725309\n",
      "Epoch: 193, Loss: 0.31229281638349804\n",
      "Epoch: 194, Loss: 0.30507449379989077\n",
      "Epoch: 195, Loss: 0.30243424006870817\n",
      "Epoch: 196, Loss: 0.32554113439151217\n",
      "Epoch: 197, Loss: 0.2999455673354013\n",
      "Epoch: 198, Loss: 0.31606925811086384\n",
      "Epoch: 199, Loss: 0.2912994772195816\n",
      "Epoch: 200, Loss: 0.31718575954437256\n",
      "Epoch: 201, Loss: 0.2870952572141375\n",
      "Epoch: 202, Loss: 0.3363852713789259\n",
      "Epoch: 203, Loss: 0.30001133041722433\n",
      "Epoch: 204, Loss: 0.288390297974859\n",
      "Epoch: 205, Loss: 0.31658964710576193\n",
      "Epoch: 206, Loss: 0.2559152458395277\n",
      "Epoch: 207, Loss: 0.2789688216788428\n",
      "Epoch: 208, Loss: 0.3188948099102293\n",
      "Epoch: 209, Loss: 0.27340331247874666\n",
      "Epoch: 210, Loss: 0.29423421195575167\n",
      "Epoch: 211, Loss: 0.2960303063903536\n",
      "Epoch: 212, Loss: 0.29319339777742115\n",
      "Epoch: 213, Loss: 0.2868918063385146\n",
      "Epoch: 214, Loss: 0.31503443845680784\n",
      "Epoch: 215, Loss: 0.27885493636131287\n",
      "Epoch: 216, Loss: 0.2736999647957938\n",
      "Epoch: 217, Loss: 0.2883776107004711\n",
      "Epoch: 218, Loss: 0.3022810327155249\n",
      "Epoch: 219, Loss: 0.2925246549504144\n",
      "Epoch: 220, Loss: 0.2939119754093034\n",
      "Epoch: 221, Loss: 0.26616301919732777\n",
      "Epoch: 222, Loss: 0.27263731190136503\n",
      "Epoch: 223, Loss: 0.3121149071625301\n",
      "Epoch: 224, Loss: 0.27867297189576284\n",
      "Epoch: 225, Loss: 0.2686523656759943\n",
      "Epoch: 226, Loss: 0.265939114349229\n",
      "Epoch: 227, Loss: 0.24403733227934157\n",
      "Epoch: 228, Loss: 0.28721889427730013\n",
      "Epoch: 229, Loss: 0.29845648791108814\n",
      "Epoch: 230, Loss: 0.2785679740565164\n",
      "Epoch: 231, Loss: 0.2724789338452475\n",
      "Epoch: 232, Loss: 0.28496833571365904\n",
      "Epoch: 233, Loss: 0.27104975496019634\n",
      "Epoch: 234, Loss: 0.27303716114589144\n",
      "Epoch: 235, Loss: 0.29159946739673615\n",
      "Epoch: 236, Loss: 0.3098713627883366\n",
      "Epoch: 237, Loss: 0.3089545411723001\n",
      "Epoch: 238, Loss: 0.297150120139122\n",
      "Epoch: 239, Loss: 0.2904440313577652\n",
      "Epoch: 240, Loss: 0.3151091124330248\n",
      "Epoch: 241, Loss: 0.2477307990193367\n",
      "Epoch: 242, Loss: 0.273243105837277\n",
      "Epoch: 243, Loss: 0.2878660389355251\n",
      "Epoch: 244, Loss: 0.3182937907321112\n",
      "Epoch: 245, Loss: 0.2510110991341727\n",
      "Epoch: 246, Loss: 0.29750255814620424\n",
      "Epoch: 247, Loss: 0.26944630060877117\n",
      "Epoch: 248, Loss: 0.24709358704941614\n",
      "Epoch: 249, Loss: 0.2700631320476532\n",
      "Epoch: 250, Loss: 0.29258492801870617\n",
      "Epoch: 251, Loss: 0.2973049432039261\n",
      "Epoch: 252, Loss: 0.25363891252449583\n",
      "Epoch: 253, Loss: 0.2853636826787676\n",
      "Epoch: 254, Loss: 0.2917328029870987\n",
      "Epoch: 255, Loss: 0.2698643846171243\n",
      "Epoch: 256, Loss: 0.28632621254239765\n",
      "Epoch: 257, Loss: 0.3050857867513384\n",
      "Epoch: 258, Loss: 0.29619583061763216\n",
      "Epoch: 259, Loss: 0.29935088966573986\n",
      "Epoch: 260, Loss: 0.2696778359157698\n",
      "Epoch: 261, Loss: 0.3276104820626123\n",
      "Epoch: 262, Loss: 0.27678766208035605\n",
      "Epoch: 263, Loss: 0.2928448809044702\n",
      "Epoch: 264, Loss: 0.26882087332861765\n",
      "Epoch: 265, Loss: 0.2542555055447987\n",
      "Epoch: 266, Loss: 0.3086821756192616\n",
      "Epoch: 267, Loss: 0.2572726735046932\n",
      "Epoch: 268, Loss: 0.2724634526031358\n",
      "Epoch: 269, Loss: 0.2529841129268919\n",
      "Epoch: 270, Loss: 0.23552332392760686\n",
      "Epoch: 271, Loss: 0.2807869613170624\n",
      "Epoch: 272, Loss: 0.28327604702540804\n",
      "Epoch: 273, Loss: 0.2744441330432892\n",
      "Epoch: 274, Loss: 0.255089385168893\n",
      "Epoch: 275, Loss: 0.24512668060404913\n",
      "Epoch: 276, Loss: 0.23668173274823598\n",
      "Epoch: 277, Loss: 0.26622065901756287\n",
      "Epoch: 278, Loss: 0.29807997814246584\n",
      "Epoch: 279, Loss: 0.266172326036862\n",
      "Epoch: 280, Loss: 0.30047751324517386\n",
      "Epoch: 281, Loss: 0.24828167685440608\n",
      "Epoch: 282, Loss: 0.2759797338928495\n",
      "Epoch: 283, Loss: 0.2512907236814499\n",
      "Epoch: 284, Loss: 0.2769182558570589\n",
      "Epoch: 285, Loss: 0.26884594559669495\n",
      "Epoch: 286, Loss: 0.2677319848111698\n",
      "Epoch: 287, Loss: 0.22999751354966844\n",
      "Epoch: 288, Loss: 0.25235662928649355\n",
      "Epoch: 289, Loss: 0.22084010392427444\n",
      "Epoch: 290, Loss: 0.2752584293484688\n",
      "Epoch: 291, Loss: 0.30110024767262594\n",
      "Epoch: 292, Loss: 0.27223007806709837\n",
      "Epoch: 293, Loss: 0.25169059208461214\n",
      "Epoch: 294, Loss: 0.28070770842688425\n",
      "Epoch: 295, Loss: 0.2330878579190799\n",
      "Epoch: 296, Loss: 0.2506941705942154\n",
      "Epoch: 297, Loss: 0.2469457941395896\n",
      "Epoch: 298, Loss: 0.23987793070929392\n",
      "Epoch: 299, Loss: 0.24555066440786635\n",
      "Epoch: 300, Loss: 0.2311283296772412\n",
      "Final-Loss = 0.2311283296772412;\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device {}.\".format(device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load the training data.\n",
    "train_loader = _get_train_data_loader(batch_size, data_dir)\n",
    "\n",
    "\n",
    "## --- Your code here --- ##\n",
    "\n",
    "## TODO:  Build the model by passing in the input params\n",
    "# To get params from the parser, call args.argument_name, ex. args.epochs or ards.hidden_dim\n",
    "# Don't forget to move your model .to(device) to move to GPU , if appropriate\n",
    "model = BinaryClassifier(input_features, hidden_dim, output_dim)\n",
    "\n",
    "## TODO: Define an optimizer and loss function for training\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Trains the model (given line of code, which calls the above training function)\n",
    "train(model, train_loader, epochs, criterion, optimizer, device)\n",
    "\n",
    "## --- End of your code  --- ##\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
